{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pNqkTlx2KE8Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "# import tenseal as ts  #for DE\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Bidirectional, LSTM, Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the wednesday dataset which contains LDOS data\n",
        "\n",
        "df = pd.read_csv('Final_data.csv')\n",
        "# df.drop(1676519)   #because it contains a row having differnt label"
      ],
      "metadata": {
        "id": "FTKAkVHRKRad"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df1 = pd.read_csv('Wednesday-workingHours.pcap_ISCX.csv')\n",
        "# # df.drop(1676519)   #because it contains a row having differnt label\n",
        "# print(df1)\n",
        "# # X = df.drop('Label', axis=1)\n",
        "# Y = df1[' Label']\n",
        "\n",
        "# print(Y.unique())\n",
        "# value_counts = Y.value_counts()\n",
        "# print(value_counts)"
      ],
      "metadata": {
        "id": "WN62-Sw341tH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing\n",
        "\n",
        "#replacing NAN values with 0\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Separating the features and label\n",
        "X = df.drop('Label', axis=1)\n",
        "Y = df['Label']\n",
        "\n",
        "non_float_cols = X.select_dtypes(exclude='float64').columns\n",
        "\n",
        "# Convert non-float columns to float, handling potential errors\n",
        "for col in non_float_cols:\n",
        "    try:\n",
        "        X[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        # print(col)\n",
        "    except ValueError:\n",
        "        print(f\"Error converting column '{col}' to float. Check for non-numeric values.\")\n",
        "\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X.fillna(0, inplace=True)\n",
        "\n",
        "# # Standardize the features\n",
        "#i dont want to do this because it contains things like ip add ports and all\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# pca for dimensionality reduction\n",
        "# pca = PCA(n_components=10)\n",
        "# X_pca = pca.fit_transform(X)\n",
        "\n",
        "# print(Y.unique()) #['attack' 'normal']\n",
        "\n",
        "# # for i in range(len(Y)):\n",
        "# #   if Y[i]==0:\n",
        "# #     print(i)\n",
        "\n",
        "#replacing the labels with 0 for non attcak and 1 for LDOS\n",
        "\n",
        "Y.replace('normal', 0, inplace=True)\n",
        "Y.replace('attack', 1, inplace=True)\n",
        "\n",
        "print(Y.unique()) # 0 1\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iatH9jnQKVlO",
        "outputId": "6dba445c-9152-4f9d-c87c-07b6a975edc7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n",
            "(2053559, 27)\n",
            "(2053559,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-dc19d36f2712>:42: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  Y.replace('attack', 1, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Split into testing set and the remaining data\n",
        "X_train_remain, X_test, Y_train_remain, Y_test = train_test_split(X, Y, test_size=300000, random_state=42, stratify=Y)\n",
        "\n",
        "print(X_train_remain.shape)\n",
        "\n",
        "# Step 2: Create the first training set for initially training the base model\n",
        "X_large_train = X_train_remain.iloc[:371852]\n",
        "Y_large_train = Y_train_remain.iloc[:371852]\n",
        "\n",
        "# Remaining data for training the federated clients\n",
        "X_remaining = X_train_remain.iloc[371852:]\n",
        "Y_remaining = Y_train_remain.iloc[371852:]\n",
        "\n",
        "print(X_remaining.shape)\n",
        "\n",
        "\n",
        "# Step 3: Create 15 smaller training sets, 3 clients 5 model aggregation iterations\n",
        "X_small_train_sets = []\n",
        "Y_small_train_sets = []\n",
        "\n",
        "# Split remaining data into 15 parts\n",
        "for i in range(15):\n",
        "    X_small_train = X_remaining.iloc[i * 100000:(i + 1) * 100000]\n",
        "    Y_small_train = Y_remaining.iloc[i * 100000:(i + 1) * 100000]\n",
        "    X_small_train_sets.append(X_small_train)\n",
        "    Y_small_train_sets.append(Y_small_train)\n",
        "\n",
        "print(X_small_train_sets[14].shape)\n",
        "\n",
        "# print(X_train)\n",
        "# print(y_train)\n",
        "\n",
        "#let us reshape consodering the timestamp\n",
        "\n",
        "X_large_train_reshaped=X_large_train.to_numpy().reshape((X_large_train.shape[0],1,X_large_train.shape[1]))\n",
        "X_train_reshaped = [None]*15\n",
        "# math.floor((i+1+2)/3)\n",
        "for i in range(15):\n",
        "  # print(i)\n",
        "  X_train_reshaped[i]=X_small_train_sets[i].to_numpy().reshape((X_small_train_sets[i].shape[0],1, X_small_train_sets[i].shape[1])) #row timestamp Columns, here timestamp will have the values 1,2,3,4,5 indicating the period in which we collected the data\n",
        "\n",
        "X_test_reshaped = X_test.to_numpy().reshape((X_test.shape[0], 1, X_test.shape[1]))#row timestamp Columns\n",
        "\n"
      ],
      "metadata": {
        "id": "CzSECZtEVee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7686c05-b2f1-40a8-cf5b-ba47b293d015"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1753559, 27)\n",
            "(1381707, 27)\n",
            "(0, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequential():\n",
        "  model = Sequential()\n",
        "  model.add(Bidirectional(LSTM(64, activation='tanh', kernel_regularizer='l2')))\n",
        "  model.add(Dense(128, activation = 'relu', kernel_regularizer='l2'))\n",
        "  model.add(Dense(1, activation = 'sigmoid', kernel_regularizer='l2'))\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "  return model\n",
        "\n",
        "def print_accuracy_loss(model,X_test,y_test):\n",
        "  loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "  print(\"Accuracy and loss is \",accuracy, loss)\n",
        "\n",
        "def print_confusion_matrix(model,X_test,y_test):\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "  cm = confusion_matrix(y_test, y_pred_binary)\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(cm)\n",
        "\n",
        "def plot_confusion_matrix(model,X_test, y_test, labels=['Attack', 'Normal']):\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "  cm = confusion_matrix(y_test, y_pred_classes)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "  disp.plot()\n",
        "  plt.show()\n",
        "\n",
        "def create_improved_sequential():\n",
        "    model = Sequential()\n",
        "    # First LSTM layer with Bidirectional and Dropout\n",
        "    model.add(Bidirectional(LSTM(128, return_sequences=True, activation='tanh', kernel_regularizer='l2'), input_shape=(X_large_train_reshaped.shape[1], X_large_train_reshaped.shape[2])))\n",
        "    model.add(Dropout(0.3))\n",
        "    # Second LSTM layer\n",
        "    model.add(Bidirectional(LSTM(64, activation='tanh', kernel_regularizer='l2')))\n",
        "    model.add(Dropout(0.3))\n",
        "    # Dense layers with Batch Normalization\n",
        "    model.add(Dense(256, activation='relu', kernel_regularizer='l2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "    # Output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compile the model\n",
        "    optimizer = RMSprop(learning_rate=0.001)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n"
      ],
      "metadata": {
        "id": "4Ao-AadhNSmc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using Homomorphic Encryption (HE) and Differential Privacy (DP)\n",
        "#in HE we send the encrypted weights and averging is performed on these encrypted weights and after averaging the weights are decrypted and then the model is updated with these weights\n",
        "#in DP we add some noise to the weights before sending it so that the plain data is not sent, more noise we add more we corrupt the data and we can control it using epsilon. smaller the epsilon more noise is induced , even if epsilon is 1 some amount of noise in induced because we use laplacian\n",
        "\n",
        "# Generate a context for HE\n",
        "# context = ts.context(ts.SCHEME_TYPE.CKKS, poly_modulus_degree=8192, coeff_mod_bit_sizes=[60, 40, 40, 60])\n",
        "# context.global_scale = 2**40\n",
        "\n",
        "#Add Noise to Weights:\n",
        "def add_dp_noise(weights, epsilon=1.0):\n",
        "    noise = [np.random.laplace(0, 1/epsilon, w.shape) for w in weights]\n",
        "    noisy_weights = [w + n for w, n in zip(weights, noise)]\n",
        "    return noisy_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "e2rpudThgWlU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Asynchronous updation\n",
        "\n",
        "def asynchronous_update(global_model,total_samples,remote_samples,remote_model_weight):\n",
        "  #total_samples- total number of samples that have been used to train\n",
        "  #remote_samples- total number of samples that have been used by the remote model to train\n",
        "\n",
        "    for i in range(len(global_model)):\n",
        "        # Compute incremental weighted average\n",
        "        global_model[i] = (\n",
        "            (global_model[i] * total_samples) + (remote_model_weight[i] * remote_samples)\n",
        "        ) / (total_samples + remote_samples)\n",
        "\n",
        "    return global_model\n",
        "\n",
        "def decay(t,choice=1):\n",
        "  b=0.01\n",
        "  if choice==1:\n",
        "    return 1/(1+math.exp(b*t))  #exponential decay\n",
        "  else:\n",
        "    return 1/(1+b*t) #linear decay\n",
        "\n",
        "def staleness_asynchronous_update(global_model,total_samples,remote_samples,remote_model_weight,t):\n",
        "  #here t represents how far from the global model is the remote model\n",
        "  for i in range(len(global_model)):\n",
        "        # Compute incremental weighted average with decay\n",
        "        global_model[i] = decay(t)*global_model[i] + (1-decay(t))*((global_model[i] * total_samples) + (remote_model_weight[i] * remote_samples)\n",
        "         / (total_samples + remote_samples))\n",
        "\n",
        "  return global_model\n"
      ],
      "metadata": {
        "id": "feu6kPFGgW4W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#training the federated architecture\n",
        "\n",
        "weightstemp=None;\n",
        "\n",
        "#no of federated clients\n",
        "remote_models_count=3\n",
        "#no of federated iterations\n",
        "federated_iterations=5\n",
        "#number of epochs\n",
        "epochs_count=1\n",
        "\n",
        "#creating a 2D list to store the models of the federated clients and to store the weights of those models\n",
        "remote_models=[[0 for _ in range(remote_models_count)] for _ in range(federated_iterations)]\n",
        "weights_remote_models=[[0 for _ in range(remote_models_count)] for _ in range(federated_iterations)]\n",
        "\n",
        "#we will use this for DP and HE\n",
        "noisy_weights=[[0 for _ in range(remote_models_count)] for _ in range(federated_iterations)]\n",
        "encrypted_weights=[[0 for _ in range(remote_models_count)] for _ in range(federated_iterations)]\n",
        "\n",
        "\n",
        "#first we train the base model and then save it so that we can forward the parameters\n",
        "base_model = create_sequential()\n",
        "history_base = base_model.fit(X_large_train_reshaped , Y_large_train, epochs = epochs_count,validation_split=0.2, verbose = 1)\n",
        "print(\"accuracy and loss before federated learning is ,\")\n",
        "# print_accuracy_loss(base_model,X_test_reshaped,Y_test)\n",
        "base_model.save('base_model.h5')\n",
        "global_model=base_model.get_weights()\n",
        "\n",
        "count=0;\n",
        "\n",
        "for i in range(federated_iterations):\n",
        "  for j in range(remote_models_count):\n",
        "    remote_models[i][j]=load_model('base_model.h5')\n",
        "    adam = tf.keras.optimizers.Adam()  # Define your optimizer\n",
        "    remote_models[i][j].compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])  # Recompile\n",
        "    print(count)\n",
        "    remote_models[i][j].fit(X_train_reshaped[count], Y_small_train_sets[count], epochs = epochs_count,validation_split=0.2, verbose = 1)\n",
        "    print(\"accuracy and loss of\",j, \"client in\",i,\" federated iteration is \")\n",
        "    # print_accuracy_loss(remote_models[i][j],X_test_reshaped,Y_test)\n",
        "    remote_models[i][j].save('remote_model'+str(i+1)+str(j+1)+'.h5')\n",
        "\n",
        "    #saving the weights of the model to our weights list\n",
        "    weights_remote_models[i][j]=remote_models[i][j].get_weights()\n",
        "\n",
        "\n",
        "    # Adding noise to these weights\n",
        "    noisy_weights[i][j] = add_dp_noise(weights_remote_models[i][j])\n",
        "\n",
        "\n",
        "    count=count+1\n",
        "\n",
        "  ##synchronous weighted average, we update the global model only after each federated iteration ie only after all the clients send their model\n",
        "  # averaged_noisy_weights=[(w1 + w2 + w3) / 3 for w1, w2, w3 in zip(noisy_weights[i][0],noisy_weights[i][2],noisy_weights[i][2])]\n",
        "  # global_model=averaged_noisy_weights\n",
        "  # base_model.set_weights(global_model)\n",
        "\n",
        "\n",
        "  #Asynchronous updation we update the global model as soon as any remote client sends his model\n",
        "    # global_model=asynchronous_update(global_model,count+1,i+1,noisy_weights[i][j])\n",
        "  #staleless Asynchronous updation we update the global model as soon as any remote client sends his model\n",
        "    global_model=staleness_asynchronous_update(global_model,count+1,i+1,noisy_weights[i][j],0)\n",
        "    base_model.set_weights(global_model)\n"
      ],
      "metadata": {
        "id": "WAQ6mSqfBw2-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "outputId": "36481bd6-c30e-4b5d-ea09-926524a60c47"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m9297/9297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5ms/step - accuracy: 0.7068 - loss: 0.6861 - val_accuracy: 0.7050 - val_loss: 0.6100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy and loss before federated learning is ,\n",
            "0\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.7102 - loss: 0.6060 - val_accuracy: 0.7045 - val_loss: 0.6113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy and loss of 0 client in 0  federated iteration is \n",
            "1\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - accuracy: 0.7081 - loss: 0.6077 - val_accuracy: 0.7078 - val_loss: 0.6072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy and loss of 1 client in 0  federated iteration is \n",
            "2\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.7076 - loss: 0.6080 - val_accuracy: 0.7079 - val_loss: 0.6095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy and loss of 2 client in 0  federated iteration is \n",
            "3\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.7082 - loss: 0.6071 - val_accuracy: 0.7124 - val_loss: 0.6031\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-08e047ec406a>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mremote_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Recompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mremote_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_reshaped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_small_train_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy and loss of\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"client in\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" federated iteration is \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# print_accuracy_loss(remote_models[i][j],X_test_reshaped,Y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"accuracy and loss after entire FL is ,\")\n",
        "print_accuracy_loss(base_model,X_test_reshaped,Y_test)\n",
        "plot_confusion_matrix(base_model,X_test_reshaped,Y_test)\n",
        "base_model.save('federated_base_model.h5')"
      ],
      "metadata": {
        "id": "Q39l_n13Up-2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}